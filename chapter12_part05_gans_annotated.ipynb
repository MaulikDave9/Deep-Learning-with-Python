{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQTMtAkjIO9A"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff), Chapter 12.\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiO02xNXIO9C"
      },
      "source": [
        "## Introduction to generative adversarial networks\n",
        "Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et al.,7 are\n",
        "an alternative to VAEs for learning latent spaces of images. They enable the generation\n",
        "of fairly realistic synthetic images by forcing the generated images to be statistically\n",
        "almost indistinguishable from real ones.\n",
        "An intuitive way to understand GANs is to imagine a forger trying to create a fake\n",
        "Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his\n",
        "fakes with authentic Picassos and shows them all to an art dealer. The art dealer makes\n",
        "an authenticity assessment for each painting and gives the forger feedback about what\n",
        "makes a Picasso look like a Picasso. The forger goes back to his studio to prepare some\n",
        "new fakes. As time goes on, the forger becomes increasingly competent at imitating\n",
        "the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes.\n",
        "In the end, they have on their hands some excellent fake Picassos.\n",
        "That’s what a GAN is: a forger network and an expert network, each being trained\n",
        "to best the other. As such, a GAN is made of two parts:\n",
        "<ul>\n",
        "    <li>Generator network—Takes as input a random vector (a random point in the\n",
        "latent space), and decodes it into a synthetic image\n",
        "<li>Discriminator network (or adversary)—Takes as input an image (real or synthetic),\n",
        "and predicts whether the image came from the training set or was created by\n",
        "the generator network\n",
        "</ul>\n",
        "\n",
        "7 Ian Goodfellow et al., “Generative Adversarial Networks,” arXiv (2014), https://arxiv.org/abs/1406.2661.\n",
        "\n",
        "The generator network is trained to be able to fool the discriminator network, and\n",
        "thus it evolves toward generating increasingly realistic images as training goes on: artificial\n",
        "images that look indistinguishable from real ones, to the extent that it’s impossible\n",
        "for the discriminator network to tell the two apart (see figure 12.19). Meanwhile,\n",
        "the discriminator is constantly adapting to the gradually improving capabilities of the\n",
        "generator, setting a high bar of realism for the generated images. Once training is\n",
        "over, the generator is capable of turning any point in its input space into a believable\n",
        "image. **Unlike VAEs, this latent space has fewer explicit guarantees of meaningful\n",
        "structure; in particular, it isn’t continuous.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DB9fNIBIO9D"
      },
      "source": [
        "**Figure 12.19** A generator transforms random latent vectors into images, and a discriminator\n",
        "seeks to tell real images from generated ones. The generator is trained to fool the discriminator\n",
        "\n",
        "Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in\n",
        "any other training setup you’ve encountered in this book. Normally, gradient descent\n",
        "consists of rolling down hills in a static loss landscape. But with a GAN, every step\n",
        "taken down the hill changes the entire landscape a little. It’s a dynamic system where\n",
        "the optimization process is seeking not a minimum, but an equilibrium between two\n",
        "forces. For this reason, GANs are notoriously difficult to train—getting a GAN to work\n",
        "requires lots of careful tuning of the model architecture and training parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-348i0FIO9D"
      },
      "source": [
        "### A schematic GAN implementation\n",
        "\n",
        "In this section, we’ll explain how to implement a GAN in Keras in its barest form.\n",
        "GANs are advanced, so diving deeply into the technical details of architectures like\n",
        "that of the StyleGAN2 that generated the images in figure 12.20 would be out of scope\n",
        "for this book. The specific implementation we’ll use in this demonstration is a deep\n",
        "convolutional GAN (DCGAN): a very basic GAN where the generator and discriminator\n",
        "are deep convnets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5zhqS9PIO9D"
      },
      "source": [
        "**Figure 12.20** Latent space dwellers. Images generated by https://thispersondoesnotexist.com\n",
        "using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The model used is the\n",
        "StyleGAN2 model from Karras et al., https://arxiv.org/abs/1912.04958.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fuV7qIIO9E"
      },
      "source": [
        "We’ll train our GAN on images from the Large-scale CelebFaces Attributes dataset\n",
        "(known as CelebA), a dataset of 200,000 faces of celebrities (http://mmlab.ie.cuhk\n",
        ".edu.hk/projects/CelebA.html) To speed up training, we’ll resize the images to 64 × 64,\n",
        "so we’ll be learning to generate 64 × 64 images of human faces.\n",
        "Schematically, the GAN looks like this:\n",
        "<ul>\n",
        "    <li>A generator network maps vectors of shape (latent_dim,) to images of shape\n",
        "(64, 64, 3).\n",
        "<li>A discriminator network maps images of shape (64, 64, 3) to a binary score\n",
        "estimating the probability that the image is real.\n",
        "<li>A gan network chains the generator and the discriminator together: gan(x) =\n",
        "discriminator(generator(x)). Thus, this gan network maps latent space vectors\n",
        "to the discriminator’s assessment of the realism of these latent vectors as\n",
        "decoded by the generator.\n",
        "<li>We train the discriminator using examples of real and fake images along with\n",
        "“real”/“fake” labels, just as we train any regular image-classification model.\n",
        "<li>To train the generator, we use the gradients of the generator’s weights with\n",
        "regard to the loss of the gan model. This means that at every step, we move the\n",
        "weights of the generator in a direction that makes the discriminator more likely\n",
        "to classify as “real” the images decoded by the generator. In other words, we\n",
        "train the generator to fool the discriminator.\n",
        "    </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i1qf6J_IO9E"
      },
      "source": [
        "### A bag of tricks\n",
        "The process of training GANs and tuning GAN implementations is notoriously difficult.\n",
        "There are a number of known tricks you should keep in mind. Like most things\n",
        "in deep learning, it’s more alchemy than science: these tricks are heuristics, not theory backed\n",
        "guidelines. They’re supported by a level of intuitive understanding of the phenomenon\n",
        "at hand, and they’re known to work well empirically, although not necessarily\n",
        "in every context.\n",
        "\n",
        "Here are a few of the tricks used in the implementation of the GAN generator and\n",
        "discriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find\n",
        "many more across the GAN literature:\n",
        "<ul>\n",
        "    <li>We use strides instead of pooling for downsampling feature maps in the discriminator,\n",
        "just like we did in our VAE encoder.\n",
        "<li>We sample points from the latent space using a normal distribution (Gaussian distribution),\n",
        "not a uniform distribution.\n",
        "<li>Stochasticity is good for inducing robustness. Because GAN training results in a\n",
        "dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing\n",
        "randomness during training helps prevent this. We introduce randomness\n",
        "by adding random noise to the labels for the discriminator.\n",
        "<li>Sparse gradients can hinder GAN training. In deep learning, sparsity is often\n",
        "a desirable property, but not in GANs. Two things can induce gradient sparsity:\n",
        "max pooling operations and relu activations. Instead of max pooling,\n",
        "we recommend using strided convolutions for downsampling, and we recommend\n",
        "using a LeakyReLU layer instead of a relu activation. It’s similar to\n",
        "relu, but it relaxes sparsity constraints by allowing small negative activation\n",
        "values.\n",
        "<li>In generated images, it’s common to see checkerboard artifacts caused by\n",
        "unequal coverage of the pixel space in the generator (see figure 12.21). To fix\n",
        "this, we use a kernel size that’s divisible by the stride size whenever we use a\n",
        "strided `Conv2DTranspose` or `Conv2D` in both the generator and the discriminator.\n",
        "   </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gpO_z7gIO9E"
      },
      "source": [
        "**Figure 12.21** Checkerboard artifacts caused by mismatching strides and kernel\n",
        "sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4fR3dXFIO9E"
      },
      "source": [
        "### Getting our hands on the CelebA dataset\n",
        "You can download the dataset manually from the website: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.\n",
        "\n",
        "If you’re using Colab, you can run the following to download\n",
        "the data from Google Drive and uncompress it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc7-YuwxIO9E"
      },
      "source": [
        "**Getting the CelebA data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUzGX9VyIO9F",
        "outputId": "f01b1a27-d6b6-4c11-d6c1-da66270ad3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\n",
            "From (redirected): https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684&confirm=t&uuid=86a7b6e9-4534-4e47-abad-766f17bd6edd\n",
            "To: /content/celeba_gan/data.zip\n",
            "100% 1.44G/1.44G [00:26<00:00, 55.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Create a working directory.\n",
        "!mkdir celeba_gan\n",
        "# Download the compressed data using gdown\n",
        "# (available by default in Colab; install it otherwise)\n",
        "!gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O celeba_gan/data.zip\n",
        "# Uncompress the data\n",
        "!unzip -qq celeba_gan/data.zip -d celeba_gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsqjiIZLIO9F"
      },
      "source": [
        "#### On Windows I went to:\n",
        "https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\n",
        "\n",
        "downloaded file 1.34 GB `imf_align_celeba.zip`.\n",
        "I unzipped the archive using 7-Zip. The expanded directory contains some 220,000 images. I shrunk the directory to the first 21,000 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S16lPDGmIO9F"
      },
      "source": [
        "**Creating a Dataset from a directory of images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cVLIgA-IO9F"
      },
      "source": [
        "Once you’ve got the uncompressed images in a directory, you can use `keras.utils.image_dataset_from_directory` to turn images into a dataset. Since we just need the image, there\n",
        "are no labels, we will specify `label_mode=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOEY559JIO9F",
        "outputId": "79bcb108-cb16-4e18-ff2c-00d7e1d2bedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    \"celeba_gan\",\n",
        "    # Only the images will be returned—no labels.\n",
        "    label_mode=None,\n",
        "    image_size=(64, 64),\n",
        "    batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaf6njQbIO9G"
      },
      "source": [
        "**Rescaling the images**\n",
        "Finally, let’s rescale the images to the [0-1] range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4epT_Bw8IO9G"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: x / 255.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drs5NT6IIO9G"
      },
      "source": [
        "**Displaying the first image**\n",
        "\n",
        "You can use the following code to display a sample image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ks6rAMbuIO9G",
        "outputId": "bd578c27-1eae-49b6-ba16-eca73694fcd6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhv34MIIO9G"
      },
      "source": [
        "### The discriminator\n",
        "First, we’ll develop a `discriminator` model that takes as input a candidate image\n",
        "(real or synthetic) and classifies it into one of two classes: “generated image” or “real\n",
        "image that comes from the training set.” One of the many issues that commonly\n",
        "arise with GANs is that the generator gets stuck with generated images that look like\n",
        "noise. A possible solution is to use dropout in the discriminator, so that’s what we\n",
        "will do here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdq-4MD5IO9G"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0N2Ny_6IO9G",
        "outputId": "7902fcff-f6f2-4ced-9cf7-f093b0f72703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        # One dropout layer: an important trick!\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "ZtX8c0q-IO9H",
        "outputId": "e90d3144-f4aa-499e-aa7a-0d7fc1528141"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"discriminator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"discriminator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m8,193\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,193</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m404,801\u001b[0m (1.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,801</span> (1.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m404,801\u001b[0m (1.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,801</span> (1.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoEITbx2IO9H"
      },
      "source": [
        "### The generator\n",
        "\n",
        "Next, let’s develop a `generator` model that turns a vector (from the latent space—\n",
        "during training it will be sampled at random) into a candidate image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IZ0s2p3IO9H"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9xM3xsxJIO9H"
      },
      "outputs": [],
      "source": [
        "# The latent space will be made of 128-dimensional vectors\n",
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # Produce the same number of coefficients we had\n",
        "        # at the level of the Flatten layer in the encoder.\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        # Revert the Flatten layer of the encoder\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        # Revert the Conv2D layers of the encoder.\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        # The output ends up with shape (28, 28, 1).\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "GTT-Opv3IO9H",
        "outputId": "c21658a5-ad34-41e8-88f4-566877844008"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │     \u001b[38;5;34m1,056,768\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,544\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │        \u001b[38;5;34m38,403\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056,768</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,403</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,979,651\u001b[0m (15.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,979,651</span> (15.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,979,651\u001b[0m (15.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,979,651</span> (15.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnZ1YibEIO9H"
      },
      "source": [
        "### The adversarial network\n",
        "Finally, we’ll set up the GAN, which chains the generator and the discriminator.\n",
        "When trained, this model will move the generator in a direction that improves its\n",
        "ability to fool the discriminator. This model turns latent-space points into a classification\n",
        "decision—“fake” or “real”—and it’s meant to be trained with labels that are\n",
        "always “these are real images.” So training gan will update the weights of generator\n",
        "in a way that makes discriminator more likely to predict “real” when looking at\n",
        "fake images.\n",
        "To recapitulate, this is what the training loop looks like schematically. For each\n",
        "epoch, you do the following:\n",
        "<ol>\n",
        "<li>Draw random points in the latent space (random noise).\n",
        "<li>Generate images with generator using this random noise.\n",
        "<li>Mix the generated images with real ones.\n",
        "<li>Train discriminator using these mixed images, with corresponding targets:\n",
        "either “real” (for the real images) or “fake” (for the generated images).\n",
        "<li>Draw new random points in the latent space.\n",
        "<li>Train generator using these random vectors, with targets that all say “these are\n",
        "real images.” This updates the weights of the generator to move them toward\n",
        "getting the discriminator to predict “these are real images” for generated\n",
        "images: this trains the generator to fool the discriminator.\n",
        "</ol>\n",
        "Let’s implement it. Like in our VAE example, we’ll use a Model subclass with a custom\n",
        "train_step(). Note that we’ll use two optimizers (one for the generator and\n",
        "one for the discriminator), so we will also override compile() to allow for passing\n",
        "two optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-AWAa-IIO9H"
      },
      "source": [
        "**The GAN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oBrvkzOOIO9H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        # Sets up metrics to track the two losses over each training epoch\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @property\n",
        "    # Sets up metrics to track the two losses over each training epoch\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        # Samples random point in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        # Combines them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "        # Assembles labels, discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Adds random noise to the labels—an important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Trains the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Samples random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "          shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assembles labels that say “these are all real images” (it’s a lie!)\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Trains the generator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oi9yCOnIO9I"
      },
      "source": [
        "Before we start training, let’s also set up a callback to monitor our results: it will use\n",
        "the generator to create and save a number of fake images at the end of each epoch.\n",
        "\n",
        "**A callback to sample generated images during training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mcTj_6-8IO9I"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.utils.array_to_img(generated_images[i])\n",
        "            img.save(f\"generated_img_{epoch:03d}_{i}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vDp6Tz-IO9I"
      },
      "source": [
        "**Compiling and training the GAN**\n",
        "\n",
        "Finally, we can start training. Please note that the following training was done on the dataset of 21,000 images, not the full set of 200,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XAgo6Z2IO9I",
        "outputId": "0d5bde85-ca01-4363-dcc8-a753c5ea60b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 81ms/step - d_loss: 0.5656 - g_loss: 1.3633\n",
            "Epoch 2/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6483 - g_loss: 1.0823\n",
            "Epoch 3/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6573 - g_loss: 1.0170\n",
            "Epoch 4/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6297 - g_loss: 1.1385\n",
            "Epoch 5/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.5792 - g_loss: 1.3067\n",
            "Epoch 6/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6047 - g_loss: 1.1403\n",
            "Epoch 7/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6397 - g_loss: 1.0234\n",
            "Epoch 8/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6566 - g_loss: 0.9459\n",
            "Epoch 9/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6691 - g_loss: 0.9244\n",
            "Epoch 10/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6647 - g_loss: 0.9285\n",
            "Epoch 11/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6656 - g_loss: 0.9276\n",
            "Epoch 12/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6583 - g_loss: 0.9356\n",
            "Epoch 13/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6629 - g_loss: 0.9202\n",
            "Epoch 14/25\n",
            "\u001b[1m6332/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 80ms/step - d_loss: 0.6606 - g_loss: 0.9343\n",
            "Epoch 15/25\n",
            "\u001b[1m5370/6332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:16\u001b[0m 80ms/step - d_loss: 0.6622 - g_loss: 0.9521"
          ]
        }
      ],
      "source": [
        "epochs = 5  # Interesting results will start appearing after epoch 20\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ivrRILIO9I"
      },
      "source": [
        "When training, you may see the adversarial loss begin to increase considerably, while\n",
        "the discriminative loss tends to zero—the discriminator may end up dominating the\n",
        "generator. If that’s the case, try reducing the discriminator learning rate, and increase\n",
        "the dropout rate of the discriminator.\n",
        "Figure 12.22 shows what our GAN is capable of generating after 30 epochs of training\n",
        "\n",
        "\n",
        "**Listing 12.38** Compiling and training the GAN.\n",
        "\n",
        "You’ll start getting\n",
        "interesting results\n",
        "after epoch 20.\n",
        "\n",
        "\n",
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qnhq3XsIO9L"
      },
      "source": [
        "**Figure 12.22** Some generated images around epoch 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z2BGJawIO9L"
      },
      "source": [
        "**Figure 12.22b** Some generated images around epoch 99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOaZzJ5ZIO9L"
      },
      "source": [
        "<ul>\n",
        "    <li>A GAN consists of a generator network coupled with a discriminator network.\n",
        "The discriminator is trained to differentiate between the output of the generator\n",
        "and real images from a training dataset, and the generator is trained to fool the discriminator. Remarkably, the generator never sees images from the training\n",
        "set directly; the information it has about the data comes from the discriminator.\n",
        "    <li>GANs are difficult to train, because training a GAN is a dynamic process rather\n",
        "than a simple gradient descent process with a fixed loss landscape. Getting a\n",
        "GAN to train correctly requires using a number of heuristic tricks, as well as\n",
        "extensive tuning.\n",
        "<li>GANs can potentially produce highly realistic images. But unlike VAEs, the\n",
        "latent space they learn doesn’t have a neat continuous structure and thus may\n",
        "not be suited for certain practical applications, such as image editing via latentspace\n",
        "concept vectors.\n",
        "</ul>\n",
        "\n",
        "These few techniques cover only the basics of this fast-expanding field. There’s a lot\n",
        "more to discover out there—generative deep learning is deserving of an entire book\n",
        "of its own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-L2u_tGIO9L"
      },
      "source": [
        "## Chapter summary\n",
        "<ul>\n",
        "    <li>You can use a sequence-to-sequence model to generate sequence data, one step\n",
        "at a time. This is applicable to text generation, but also to note-by-note music\n",
        "generation or any other type of timeseries data.\n",
        "<li>DeepDream works by maximizing convnet layer activations through gradient\n",
        "ascent in input space.\n",
        "<li>In the style-transfer algorithm, a content image and a style image are combined\n",
        "together via gradient descent to produce an image with the high-level features\n",
        "of the content image and the local characteristics of the style image.\n",
        "<li>VAEs and GANs are models that learn a latent space of images and can then\n",
        "dream up entirely new images by sampling from the latent space. Concept vectors\n",
        "in the latent space can even be used for image editing.\n",
        "    </ul>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}